import logging
import optuna
import json
import numpy as np
import pandas as pd
from sqlalchemy.orm import Session
from sqlalchemy import text
from datetime import datetime, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
import asyncio

# Importy wewnƒôtrzne
from .. import models
from . import backtest_engine
from .utils import (
    update_system_control, 
    append_scan_log, 
    get_optimized_periods_v4,
    standardize_df_columns, 
    calculate_atr,
    calculate_h3_metrics_v4  # Pe≈Çna logika V5
)
from . import aqm_v3_metrics 
from .apex_audit import SensitivityAnalyzer
from ..database import get_db_session 

logger = logging.getLogger(__name__)
optuna.logging.set_verbosity(optuna.logging.WARNING)

class QuantumOptimizer:
    """
    SERCE SYSTEMU APEX V4/V5 - PRZYSPIESZENIE 20x+
    - R√≥wnoleg≈Ça optymalizacja bayesowska
    - Cache'owanie danych w pamiƒôci RAM
    - U≈ªYWA PE≈ÅNEJ LOGIKI H3 (J - m^2 - nabla^2) DLA ZGODNO≈öCI Z FAZƒÑ 3
    - FIX LOG√ìW: Pe≈Çna transparentno≈õƒá procesu w UI
    - FIX LOGIKI (V5.4): m_sq ignoruje newsy (normalized_news = 0.0), J uwzglƒôdnia newsy.
    """

    def __init__(self, session: Session, job_id: str, target_year: int):
        self.session = session 
        self.job_id = job_id
        self.target_year = target_year
        self.study = None
        self.best_score_so_far = -1.0
        self.data_cache = {}  
        
        logger.info(f"QuantumOptimizer V5 initialized for Job {job_id}")

    def run(self, n_trials: int = 1000):
        """
        Uruchamia g≈Ç√≥wny proces optymalizacji.
        """
        start_msg = f"üöÄ QUANTUM OPTIMIZER V5: Start {self.job_id} (Rok: {self.target_year}, Pr√≥by: {n_trials})"
        logger.info(start_msg)
        append_scan_log(self.session, start_msg)
        update_system_control(self.session, 'worker_status', 'OPTIMIZING_INIT')
        
        job = self.session.query(models.OptimizationJob).filter(models.OptimizationJob.id == self.job_id).first()
        if job:
            job.status = 'RUNNING'
            self.session.commit()
        
        try:
            # KROK 1: ≈Åadowanie danych (z logowaniem postƒôpu)
            self._preload_data_to_cache()
            
            # KROK 2: Optymalizacja
            update_system_control(self.session, 'worker_status', 'OPTIMIZING_CALC')
            msg_calc = "‚úÖ Dane w pamiƒôci. Uruchamianie algorytmu genetycznego Optuna..."
            logger.info(msg_calc)
            append_scan_log(self.session, msg_calc)

            self.study = optuna.create_study(
                direction='maximize',
                sampler=optuna.samplers.TPESampler(
                    n_startup_trials=min(50, max(10, int(n_trials/5))), # Dynamiczny startup
                    multivariate=True,
                    group=True
                )
            )
            
            self.study.optimize(
                self._objective, 
                n_trials=n_trials,
                catch=(Exception,),
                show_progress_bar=False
            )
            
            # KROK 3: Zapis wynik√≥w
            if len(self.study.trials) == 0:
                raise Exception("Brak udanych pr√≥b optymalizacji (0 trials completed).")

            best_trial = self.study.best_trial
            best_value = float(best_trial.value)
            
            end_msg = f"üèÅ QUANTUM OPTIMIZER V5: Zako≈Ñczono! Najlepszy Score: {best_value:.4f}"
            logger.info(end_msg)
            append_scan_log(self.session, end_msg)
            
            safe_params = {k: float(v) if isinstance(v, (np.floating, float)) else v for k, v in best_trial.params.items()}
            append_scan_log(self.session, f"üèÜ Zwyciƒôskie Parametry:\n{json.dumps(safe_params, indent=2)}")

            # KROK 4: Analiza Wra≈ºliwo≈õci
            append_scan_log(self.session, "üìä Generowanie analizy wra≈ºliwo≈õci...")
            trials_data = self._collect_trials_data()
            sensitivity_report = self._run_sensitivity_analysis(trials_data)
            
            self._finalize_job(best_trial, sensitivity_report)
            append_scan_log(self.session, "‚úÖ Zadanie zako≈Ñczone pomy≈õlnie.")

        except Exception as e:
            self.session.rollback()
            error_msg = f"‚ùå QUANTUM OPTIMIZER V5 AWARIA: {str(e)}"
            logger.error(error_msg, exc_info=True)
            append_scan_log(self.session, error_msg)
            self._mark_job_failed()
            raise

    def _preload_data_to_cache(self):
        """≈Åaduje dane do cache RAM i raportuje postƒôp do UI"""
        update_system_control(self.session, 'worker_status', 'OPTIMIZING_DATA_LOAD')
        msg = "üîÑ PRZYSPIESZENIE: Rozpoczynam ≈Çadowanie danych i pre-kalkulacjƒô H3..."
        logger.info(msg)
        append_scan_log(self.session, msg)
        
        tickers = self._get_all_tickers()
        
        if not tickers:
            msg_err = "‚ö†Ô∏è OSTRZE≈ªENIE: Brak ticker√≥w w bazie danych! Optymalizacja nie ma na czym pracowaƒá."
            logger.warning(msg_err)
            append_scan_log(self.session, msg_err)
            return

        # Ograniczamy tickery dla wydajno≈õci, ale informujemy o tym
        tickers_to_load = tickers[:200] 
        append_scan_log(self.session, f"Znaleziono {len(tickers)} ticker√≥w. ≈Åadowanie {len(tickers_to_load)} najaktywniejszych do pamiƒôci RAM...")

        loaded_count = 0
        with ThreadPoolExecutor(max_workers=5) as executor: 
            futures = []
            for ticker in tickers_to_load:
                futures.append(executor.submit(self._load_ticker_data, ticker))
            
            for i, future in enumerate(as_completed(futures)):
                try:
                    future.result()
                    loaded_count += 1
                    # Raportuj co 50 ticker√≥w, ≈ºeby nie spamowaƒá, ale dawaƒá znak ≈ºycia
                    if loaded_count % 50 == 0:
                        append_scan_log(self.session, f"   ... za≈Çadowano {loaded_count}/{len(tickers_to_load)} ticker√≥w")
                except Exception as e:
                    logger.warning(f"B≈ÇƒÖd w wƒÖtku ≈Çadowania: {e}")
        
        msg_done = f"‚úÖ Cache gotowy. Za≈Çadowano {len(self.data_cache)} pe≈Çnych zestaw√≥w danych."
        logger.info(msg_done)
        append_scan_log(self.session, msg_done)

    def _load_ticker_data(self, ticker):
        """≈Åaduje i PRZETWARZA PE≈ÅNE DANE dla tickera"""
        # U≈ºywamy osobnej sesji dla wƒÖtku
        with get_db_session() as thread_session:
            try:
                api_client = backtest_engine.AlphaVantageClient()
                
                # 1. Pobierz dane surowe
                daily_data = get_raw_data_with_cache(
                    thread_session, api_client, ticker, 
                    'DAILY_OHLCV', 'get_time_series_daily', outputsize='full'
                )
                h2_data = aqm_v3_h2_loader.load_h2_data_into_cache(ticker, api_client, thread_session)
                
                if daily_data and h2_data:
                    # 2. WSTƒòPNE PRZETWARZANIE (Parsing + Full H3 Logic)
                    processed_df = self._preprocess_ticker_full_h3(daily_data, h2_data)
                    
                    if not processed_df.empty:
                        self.data_cache[ticker] = processed_df
                        
            except Exception as e:
                logger.error(f"B≈ÇƒÖd w wƒÖtku load_ticker_data ({ticker}): {e}")

    def _get_all_tickers(self):
        """Pobiera tickery"""
        try:
            query = text("""
                (SELECT ticker FROM phase1_candidates)
                UNION 
                (SELECT ticker FROM portfolio_holdings)
                UNION
                (SELECT ticker FROM companies LIMIT 300)
                LIMIT 300
            """)
            result = self.session.execute(query)
            return [r[0] for r in result]
        except Exception as e:
            logger.error(f"B≈ÇƒÖd pobierania ticker√≥w: {e}")
            return []

    def _preprocess_ticker_full_h3(self, daily_data, h2_data) -> pd.DataFrame:
        """
        TWORZY PE≈ÅNY DATAFRAME ZGODNY Z FAZƒÑ 3 (LIVE).
        U≈ºywa calculate_h3_metrics_v4 z utils.py.
        """
        try:
            # 1. Standardyzacja OHLC
            daily_df = standardize_df_columns(
                pd.DataFrame.from_dict(daily_data.get('Time Series (Daily)', {}), orient='index')
            )
            if len(daily_df) < 100: return pd.DataFrame()
            
            daily_df.index = pd.to_datetime(daily_df.index)
            daily_df.sort_index(inplace=True)

            # 2. Podstawowe wska≈∫niki
            daily_df['atr_14'] = calculate_atr(daily_df).ffill().fillna(0)
            daily_df['price_gravity'] = (daily_df['high'] + daily_df['low'] + daily_df['close']) / 3 / daily_df['close'] - 1
            
            # 3. Integracja danych H2
            insider_df = h2_data.get('insider_df')
            news_df = h2_data.get('news_df')
            
            daily_df['institutional_sync'] = daily_df.apply(lambda row: aqm_v3_metrics.calculate_institutional_sync_from_data(insider_df, row.name), axis=1)
            daily_df['retail_herding'] = daily_df.apply(lambda row: aqm_v3_metrics.calculate_retail_herding_from_data(news_df, row.name), axis=1)
            
            # 4. Metryki po≈õrednie
            daily_df['daily_returns'] = daily_df['close'].pct_change()
            daily_df['market_temperature'] = daily_df['daily_returns'].rolling(window=30).std()
            
            if not news_df.empty:
                news_counts = news_df.groupby(news_df.index.date).size()
                news_counts.index = pd.to_datetime(news_counts.index)
                news_counts = news_counts.reindex(daily_df.index, fill_value=0)
                daily_df['information_entropy'] = news_counts.rolling(window=10).sum()
            else:
                daily_df['information_entropy'] = 0.0
            
            # Obliczanie m_sq (Zgodnie z Backtestem V4 Original - zerujemy normalized_news w m_sq)
            daily_df['avg_volume_10d'] = daily_df['volume'].rolling(window=10).mean()
            daily_df['vol_mean_200d'] = daily_df['avg_volume_10d'].rolling(window=200).mean()
            daily_df['vol_std_200d'] = daily_df['avg_volume_10d'].rolling(window=200).std()
            daily_df['normalized_volume'] = ((daily_df['avg_volume_10d'] - daily_df['vol_mean_200d']) / daily_df['vol_std_200d']).replace([np.inf, -np.inf], 0).fillna(0)
            
            # KLUCZOWA ZMIANA: WYMUSZONE 0.0 dla NEWS√ìW W MASIE
            # To sprawia, ≈ºe Optimizer "trenuje na czysto" i znajduje wiƒôcej okazji
            daily_df['normalized_news'] = 0.0 
            
            daily_df['m_sq'] = daily_df['normalized_volume'] + daily_df['normalized_news']
            daily_df['nabla_sq'] = daily_df['price_gravity']

            # 5. FULL H3 CALCULATION (Importowana funkcja)
            # Przekazujemy puste params, bo percentyl liczymy dynamicznie w _run_fast_simulation
            daily_df = calculate_h3_metrics_v4(daily_df, {}) 
            
            return daily_df.fillna(0)
            
        except Exception as e:
            # logger.error(f"B≈ÇƒÖd preprocessingu dla Optimzera: {e}") # Unikamy spamu w logach
            return pd.DataFrame()

    def _objective(self, trial):
        """FUNKCJA CELU"""
        params = {
            'h3_percentile': trial.suggest_float('h3_percentile', 0.85, 0.98),
            'h3_m_sq_threshold': trial.suggest_float('h3_m_sq_threshold', -1.0, 0.0),
            'h3_min_score': trial.suggest_float('h3_min_score', 0.0, 2.0),
            'h3_tp_multiplier': trial.suggest_float('h3_tp_multiplier', 3.0, 8.0),
            'h3_sl_multiplier': trial.suggest_float('h3_sl_multiplier', 1.5, 4.0),
            'h3_max_hold': trial.suggest_int('h3_max_hold', 3, 10),
        }

        start_date = f"{self.target_year}-01-01"
        end_date = f"{self.target_year}-12-31"
        
        try:
            sim_res = self._run_fast_simulation(params, start_date, end_date)
            pf = sim_res.get('profit_factor', 0.0)
            trades = sim_res.get('total_trades', 0)
            
            if trades < 50: return 0.0 
            
            final_score = pf 
            
            # Logowanie postƒôpu do UI co 10 pr√≥b
            if trial.number % 10 == 0:
                log_msg = f"üî∏ Pr√≥ba {trial.number}: PF={pf:.2f}, Trades={trades}"
                # logger.info(log_msg)
                append_scan_log(self.session, log_msg)

            if final_score > self.best_score_so_far:
                self.best_score_so_far = final_score
                self._update_best_score(final_score)

            self._save_trial(trial, params, pf, trades, final_score)
            return float(final_score)

        except Exception as e:
            return 0.0

    def _run_fast_simulation(self, params, start_date, end_date):
        trades_results = []
        
        tickers = list(self.data_cache.keys())
        
        for ticker in tickers:
            df = self.data_cache[ticker]
            if df.empty: continue
            
            try:
                # DYNAMICZNE OBLICZANIE PROGU na podstawie prekomputowanego aqm_score_h3
                current_thresholds = df['aqm_score_h3'].rolling(window=100).quantile(params['h3_percentile'])
                
                start_idx = df.index.searchsorted(pd.Timestamp(start_date))
                end_idx = df.index.searchsorted(pd.Timestamp(end_date))
                
                if start_idx >= end_idx: continue
                
                # Iteracja po dniach
                for i in range(start_idx, min(end_idx, len(df) - 1)):
                    score = df.iloc[i]['aqm_score_h3']
                    threshold = current_thresholds.iloc[i]
                    m_norm = df.iloc[i]['m_sq_norm'] 
                    
                    if pd.isna(score) or pd.isna(threshold): continue
                    
                    if (score > threshold) and \
                       (m_norm < params['h3_m_sq_threshold']) and \
                       (score > params['h3_min_score']):
                        
                        entry_price = df.iloc[i + 1]['open']
                        atr = df.iloc[i]['atr_14']
                        
                        if pd.isna(entry_price) or atr == 0: continue
                        
                        tp = entry_price + params['h3_tp_multiplier'] * atr
                        sl = entry_price - params['h3_sl_multiplier'] * atr
                        
                        pnl = 0.0
                        for j in range(1, params['h3_max_hold'] + 1):
                            if i + j >= len(df): break
                            candle = df.iloc[i + j]
                            
                            if candle['low'] <= sl:
                                pnl = (sl - entry_price) / entry_price
                                break
                            elif candle['high'] >= tp:
                                pnl = (tp - entry_price) / entry_price
                                break
                            elif j == params['h3_max_hold']:
                                pnl = (candle['close'] - entry_price) / entry_price
                        
                        trades_results.append(pnl * 100)
                        
            except Exception:
                continue

        return self._calculate_stats(trades_results)

    def _calculate_stats(self, trades):
        if not trades: return {'profit_factor': 0.0, 'total_trades': 0}
        wins = [t for t in trades if t > 0]
        losses = [t for t in trades if t <= 0]
        total_win = sum(wins)
        total_loss = abs(sum(losses))
        pf = total_win / total_loss if total_loss > 0 else 0.0
        return {'profit_factor': pf, 'total_trades': len(trades), 'net_profit': sum(trades)}

    def _collect_trials_data(self):
        trials_data = []
        for t in self.study.trials:
            if t.state == optuna.trial.TrialState.COMPLETE:
                safe_params = {k: float(v) if isinstance(v, (np.floating, float)) else v for k, v in t.params.items()}
                trials_data.append({'params': safe_params, 'profit_factor': float(t.value) if t.value is not None else 0.0})
        return trials_data

    def _run_sensitivity_analysis(self, trials_data):
        if len(trials_data) < 20: return {}
        try:
            analyzer = SensitivityAnalyzer()
            return analyzer.analyze_parameter_sensitivity(trials_data)
        except Exception as e:
            logger.error(f"B≈ÇƒÖd analizy wra≈ºliwo≈õci: {e}")
            return {}

    def _update_best_score(self, score):
        try:
            job = self.session.query(models.OptimizationJob).filter(models.OptimizationJob.id == self.job_id).first()
            if job:
                job.best_score = float(score) if score is not None else 0.0
                self.session.commit()
        except Exception: self.session.rollback()

    def _save_trial(self, trial, params, pf, trades, score):
        try:
            safe_pf = float(pf) if pf is not None and not np.isnan(pf) else 0.0
            safe_trades = int(trades) if trades is not None else 0
            safe_score = float(score) if score is not None and not np.isnan(score) else 0.0
            safe_params = {k: float(v) if isinstance(v, (np.floating, float)) else v for k, v in params.items()}

            trial_record = models.OptimizationTrial(
                job_id=self.job_id, trial_number=trial.number, params=safe_params,
                profit_factor=safe_pf, total_trades=safe_trades, win_rate=0.0, net_profit=0.0,
                state='COMPLETE' if safe_score > 0 else 'PRUNED', created_at=datetime.now(timezone.utc)
            )
            self.session.add(trial_record)
            if trial.number % 10 == 0: self.session.commit()
        except Exception: self.session.rollback()

    def _finalize_job(self, best_trial, sensitivity_report):
        job = self.session.query(models.OptimizationJob).filter(models.OptimizationJob.id == self.job_id).first()
        if job:
            job.status = 'COMPLETED'
            job.best_score = float(best_trial.value) if best_trial.value is not None else 0.0
            best_params_safe = {k: float(v) if isinstance(v, (np.floating, float)) else v for k, v in best_trial.params.items()}
            final_config = {'best_params': best_params_safe, 'sensitivity_analysis': sensitivity_report, 'optimization_version': 'V5_FULL_PHYSICS', 'total_trials_processed': len(self.study.trials)}
            job.configuration = final_config
            self.session.commit()

    def _mark_job_failed(self):
        try:
            job = self.session.query(models.OptimizationJob).filter(models.OptimizationJob.id == self.job_id).first()
            if job: job.status = 'FAILED'; self.session.commit()
        except: self.session.rollback()
