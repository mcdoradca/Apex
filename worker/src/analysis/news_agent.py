import logging
import hashlib
import time
from datetime import datetime, timedelta
from sqlalchemy.orm import Session
from sqlalchemy import text, select, func

# Importy modeli
from ..models import TradingSignal, ProcessedNews, PortfolioHolding, PhaseXCandidate

# Importy narzƒôdziowe
from ..analysis.utils import update_system_control, get_system_control_value, send_telegram_alert, append_scan_log
from ..data_ingestion.alpha_vantage_client import AlphaVantageClient

logger = logging.getLogger(__name__)

# ==================================================================
# KONFIGURACJA AGENTA (ZGODNA Z SUGESTIAMI SUPPORTU AV)
# ==================================================================

BATCH_SIZE = 15                 # Ma≈Çe paczki, ≈ºeby uniknƒÖƒá Rate Limit
LOOKBACK_HOURS = 72             # Okno przesuwne (zamiast sztywnego last_scan)
LIMIT_PER_REQ = 50              # Wiƒôcej news√≥w na request

# Nowe Progi (Wy≈ºsze, ale pewniejsze)
MIN_RELEVANCE_SCORE = 0.50      
MIN_SENTIMENT_SCORE = 0.25      

# ==================================================================
# FUNKCJE POMOCNICZE
# ==================================================================

def _create_smart_hash(ticker: str, headline: str, source: str) -> str:
    """
    Deduplikacja Hybrydowa: Ticker + Tytu≈Ç + ≈πr√≥d≈Ço.
    Ignoruje URL (kt√≥ry mo≈ºe siƒô zmieniaƒá) i czas (kt√≥ry mo≈ºe byƒá przesuniƒôty).
    """
    # Normalizacja string√≥w (ma≈Çe litery, bez spacji na ko≈Ñcach)
    s = f"{ticker.strip().upper()}|{headline.strip().lower()}|{source.strip().lower()}"
    return hashlib.sha256(s.encode('utf-8')).hexdigest()

def _check_if_news_processed(session: Session, ticker: str, news_hash: str) -> bool:
    """Sprawdza w bazie, czy ten konkretny news (hash tre≈õci) ju≈º by≈Ç."""
    try:
        # Sprawdzamy historiƒô z 7 dni (≈ºeby nie trzymaƒá wiecznie hashy)
        seven_days_ago = datetime.utcnow() - timedelta(days=7)
        exists = session.scalar(
            select(func.count(ProcessedNews.id))
            .where(ProcessedNews.ticker == ticker)
            .where(ProcessedNews.news_hash == news_hash)
            .where(ProcessedNews.processed_at >= seven_days_ago)
        )
        return exists > 0
    except Exception as e:
        logger.error(f"Agent Newsowy: B≈ÇƒÖd DB (Check Dup): {e}")
        return False 

def _save_processed_news(session: Session, ticker: str, news_hash: str, sentiment: str, headline: str, url: str):
    """Rejestruje news w bazie, aby nie wys≈Çaƒá go drugi raz."""
    try:
        entry = ProcessedNews(
            ticker=ticker,
            news_hash=news_hash,
            sentiment=sentiment,
            headline=headline[:1000] if headline else "",
            source_url=url[:1000] if url else ""
        )
        session.add(entry)
        session.commit()
    except Exception as e:
        logger.error(f"Agent Newsowy: B≈ÇƒÖd DB (Insert): {e}")
        session.rollback()

# ==================================================================
# G≈Å√ìWNA FUNKCJA WORKERA
# ==================================================================

def run_news_agent_cycle(session: Session, api_client: AlphaVantageClient):
    """
    Wykonywane przez Schedule w main.py.
    """
    # Log startowy
    logger.info(f"[NewsAgent] >>> START CYKLU (Window: {LOOKBACK_HOURS}h, Rel>{MIN_RELEVANCE_SCORE})")

    try:
        # 1. Pobieranie Ticker√≥w
        try:
            phasex_tickers = set(session.scalars(select(PhaseXCandidate.ticker)).all())
            active_signals = session.scalars(select(TradingSignal.ticker).where(TradingSignal.status.in_(['ACTIVE', 'PENDING']))).all()
            portfolio_tickers = set(session.scalars(select(PortfolioHolding.ticker)).all())
        except Exception as db_err:
            logger.error(f"[NewsAgent] B≈ÇƒÖd bazy danych przy pobieraniu ticker√≥w: {db_err}")
            return

        # ≈ÅƒÖczenie list
        all_tickers = list(phasex_tickers.union(set(active_signals)).union(portfolio_tickers))
        all_tickers.sort()

        if not all_tickers:
            logger.warning("[NewsAgent] Lista ticker√≥w jest PUSTA. Nic do roboty.")
            return

        # 2. Testowy Ping Telegrama (Tylko raz na uruchomienie workera, mo≈ºna sterowaƒá flagƒÖ w bazie, ale tu zrobimy zawsze przy starcie cyklu DEBUGOWO)
        # Aby nie spamowaƒá, sprawdzamy czy wys≈Çali≈õmy ju≈º "ping" w ciƒÖgu ostatniej godziny - mo≈ºna to pominƒÖƒá je≈õli chcesz widzieƒá test za ka≈ºdym razem.
        # logger.info("[NewsAgent] Wysy≈Çanie testowego pinga na Telegram...")
        # send_telegram_alert("üì° <b>NewsAgent Heartbeat</b>: System nas≈Çuchuje.")

        # 3. Konfiguracja Czasu (Sliding Window)
        # ZAWSZE bierzemy ostatnie 72h. Deduplikacja w bazie (ProcessedNews) zadba o to, ≈ºeby nie dublowaƒá alert√≥w.
        time_from_str = (datetime.utcnow() - timedelta(hours=LOOKBACK_HOURS)).strftime('%Y%m%dT%H%M')

        processed_count = 0
        
        # 4. Batching
        batches = [all_tickers[i:i + BATCH_SIZE] for i in range(0, len(all_tickers), BATCH_SIZE)]
        total_batches = len(batches)

        for i, batch in enumerate(batches):
            ticker_string = ",".join(batch)
            
            # Log postƒôpu
            if i % 5 == 0:
                logger.info(f"[NewsAgent] Batch {i+1}/{total_batches} ({len(batch)} ticker√≥w)...")

            try:
                # API CALL
                news_data = api_client.get_news_sentiment(
                    ticker=ticker_string, 
                    limit=LIMIT_PER_REQ, 
                    time_from=time_from_str
                )
            except Exception as e:
                logger.error(f"[NewsAgent] API Error Batch {i+1}: {e}")
                continue

            # Sleep 1.5s (Rate Limit Guard)
            if len(batches) > 1:
                time.sleep(1.5)

            if not news_data or 'feed' not in news_data:
                # To nie b≈ÇƒÖd, po prostu brak news√≥w w oknie 72h dla tych sp√≥≈Çek
                continue

            # 5. Analiza Feeda
            for item in news_data.get('feed', []):
                headline = item.get('title', 'No Title')
                url = item.get('url', '#')
                source = item.get('source', 'Unknown')
                ticker_sentiment_list = item.get('ticker_sentiment', [])
                
                if not ticker_sentiment_list: continue

                # Hot Topics
                topics = item.get('topics', [])
                topic_tags = []
                is_hot_topic = False
                for t in topics:
                    t_name = t.get('topic', '')
                    if t_name in ['Earnings', 'Mergers & Acquisitions', 'Life Sciences']:
                        topic_tags.append(t_name)
                        is_hot_topic = True

                for ts_data in ticker_sentiment_list:
                    ticker = ts_data.get('ticker')
                    
                    if ticker not in all_tickers:
                        continue

                    # === NOWA DEDUPLIKACJA ===
                    # Hashujemy: Ticker + Tytu≈Ç + ≈πr√≥d≈Ço (omijamy URL i czas)
                    news_hash = _create_smart_hash(ticker, headline, source)
                    
                    if _check_if_news_processed(session, ticker, news_hash):
                        # Ju≈º to widzieli≈õmy -> SKIP
                        continue

                    try:
                        relevance_score = float(ts_data.get('relevance_score', 0))
                        sentiment_score = float(ts_data.get('ticker_sentiment_score', 0))
                        sentiment_label = ts_data.get('ticker_sentiment_label', 'Neutral')
                    except: continue

                    # === FILTRY (STRICTER) ===
                    
                    # 1. Relevance > 0.5 (chyba ≈ºe Hot Topic)
                    req_relevance = MIN_RELEVANCE_SCORE
                    if is_hot_topic: req_relevance = 0.3 # Dla earnings√≥w bierzemy szersze spektrum
                    
                    if relevance_score < req_relevance:
                        # logger.debug(f"SKIP {ticker}: Rel {relevance_score:.2f} < {req_relevance}")
                        continue

                    # 2. Sentiment > 0.25 (lub Hot Topic)
                    req_sentiment = MIN_SENTIMENT_SCORE
                    if is_hot_topic: req_sentiment = 0.1 # Earningsy sƒÖ wa≈ºne nawet przy neutralnym sentymencie

                    if abs(sentiment_score) < req_sentiment:
                        # logger.debug(f"SKIP {ticker}: Sent {sentiment_score:.2f} too weak")
                        continue

                    # === ACTION: NOTIFICATION ===
                    
                    # 1. Zapis do DB (≈ºeby nie wys≈Çaƒá ponownie)
                    # U≈ºywamy etykiety alertu, np. "BULLISH_NEWS"
                    alert_type = "NEWS"
                    if sentiment_score > 0.3: alert_type = "STRONG_BUY_NEWS"
                    elif sentiment_score < -0.3: alert_type = "STRONG_SELL_NEWS"

                    _save_processed_news(session, ticker, news_hash, alert_type, headline, url)

                    # 2. Formatowanie Wiadomo≈õci
                    icon = "üì∞"
                    if sentiment_score > 0.4: icon = "üöÄ"
                    elif sentiment_score < -0.4: icon = "üîª"
                    elif is_hot_topic: icon = "üî•"

                    msg_body = (
                        f"{icon} <b>NEWS ALERT: {ticker}</b>\n"
                        f"Tytu≈Ç: {headline}\n"
                        f"Sentyment: {sentiment_label} ({sentiment_score:.2f})\n"
                        f"Relevance: {relevance_score:.2f} | ≈πr√≥d≈Ço: {source}\n"
                        f"{url}"
                    )

                    # 3. Log Systemowy
                    log_entry = f"NEWS DETECTED: {ticker} (Sc:{sentiment_score:.2f}) | {headline[:50]}"
                    logger.info(f"‚úÖ {log_entry}")
                    append_scan_log(session, log_entry)

                    # 4. TELEGRAM (CRITICAL PATH)
                    try:
                        send_telegram_alert(msg_body)
                        # Log success
                        logger.info(f"Telegram sent for {ticker}")
                    except Exception as tele_err:
                        logger.error(f"Telegram FAILED for {ticker}: {tele_err}")

                    processed_count += 1

        if processed_count > 0:
            logger.info(f"[NewsAgent] Zako≈Ñczono. Wys≈Çano {processed_count} alert√≥w.")
        else:
            logger.info("[NewsAgent] Zako≈Ñczono. Brak nowych alert√≥w (wszystko przefiltrowane lub duplikaty).")

    except Exception as e:
        logger.error(f"[NewsAgent] B≈ÇƒÖd krytyczny cyklu: {e}", exc_info=True)
        session.rollback()
