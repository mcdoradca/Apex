import logging
import time
import hashlib
from datetime import datetime, timedelta
from sqlalchemy.orm import Session
from sqlalchemy import text, select, func

# Modele bazy danych
from ..models import ProcessedNews, PhaseXCandidate

# Importy narzÄ™dziowe
from .utils import (
    append_scan_log, 
    send_telegram_alert, 
    get_raw_data_with_cache,
    update_system_control
)
# MÃ³zg Agenta Newsowego
from .ai_agents import _run_news_analysis_agent

logger = logging.getLogger(__name__)

# ==================================================================
# NARZÄ˜DZIA POMOCNICZE
# ==================================================================

def _create_news_hash(headline: str, uri: str) -> str:
    s = f"{headline.strip()}{uri.strip()}"
    return hashlib.sha256(s.encode('utf-8')).hexdigest()

def _is_news_processed(session: Session, ticker: str, news_hash: str) -> bool:
    try:
        seven_days_ago = datetime.utcnow() - timedelta(days=7)
        exists = session.scalar(
            select(func.count(ProcessedNews.id))
            .where(ProcessedNews.ticker == ticker)
            .where(ProcessedNews.news_hash == news_hash)
            .where(ProcessedNews.processed_at >= seven_days_ago)
        )
        return exists > 0
    except Exception:
        return False

def _register_processed_news(session: Session, ticker: str, news_hash: str, sentiment: str, headline: str, url: str):
    try:
        entry = ProcessedNews(
            ticker=ticker,
            news_hash=news_hash,
            sentiment=sentiment,
            headline=headline[:1000] if headline else "",
            source_url=url[:1000] if url else ""
        )
        session.add(entry)
        session.commit()
    except Exception as e:
        session.rollback()

# ==================================================================
# CZÄ˜ÅšÄ† 1: LIVE MONITOR (StraÅ¼nik BioX - 5 min check)
# ==================================================================

def run_biox_live_monitor(session: Session, api_client):
    """
    StraÅ¼nik BioX. Monitoruje listÄ™ kandydatÃ³w Fazy X.
    Wersja VERBOSE - raportuje aktywnoÅ›Ä‡ w UI.
    """
    # 1. Pobierz listÄ™ tickerÃ³w Fazy X
    try:
        tickers_rows = session.execute(text("SELECT ticker FROM phasex_candidates")).fetchall()
        tickers = [r[0] for r in tickers_rows]
    except Exception as e:
        logger.error(f"BioX Live: BÅ‚Ä…d bazy: {e}")
        return

    if not tickers:
        # JeÅ›li lista pusta, milczymy lub dajemy znaÄ‡ raz na jakiÅ› czas
        return

    # LOG STARTOWY (Dla widocznoÅ›ci w UI)
    start_msg = f"ğŸ•µï¸ BioX Agent: Start cyklu. MonitorujÄ™ {len(tickers)} spÃ³Å‚ek Biotech..."
    logger.info(start_msg)
    append_scan_log(session, start_msg)

    chunk_size = 50
    processed_news_count = 0
    alerts_sent = 0
    
    for i in range(0, len(tickers), chunk_size):
        chunk = tickers[i:i + chunk_size]
        tickers_str = ",".join(chunk)
        
        try:
            # 2. Pobierz NEWSY (Premium Endpoint)
            news_response = api_client.get_news_sentiment(ticker=tickers_str, limit=50)
            
            if not news_response or 'feed' not in news_response:
                time.sleep(1)
                continue
                
            for item in news_response.get('feed', []):
                headline = item.get('title', '')
                summary = item.get('summary', '')
                url = item.get('url', '')
                
                if not headline: continue

                relevant_ticker = None
                for topic in item.get('topics', []):
                    if topic['ticker'] in chunk:
                        relevant_ticker = topic['ticker']
                        break
                
                if not relevant_ticker: continue

                # SprawdÅº duplikaty
                news_hash = _create_news_hash(headline, url)
                if _is_news_processed(session, relevant_ticker, news_hash):
                    continue

                # === ANALIZA AI ===
                ai_verdict = _run_news_analysis_agent(relevant_ticker, headline, summary, url)
                sentiment = ai_verdict.get('sentiment', 'NEUTRAL')
                reason = ai_verdict.get('reason', 'Brak analizy')
                
                _register_processed_news(session, relevant_ticker, news_hash, sentiment, headline, url)
                processed_news_count += 1

                # Logika PowiadomieÅ„
                if sentiment == 'CRITICAL_POSITIVE':
                    alerts_sent += 1
                    alert_msg = (
                        f"ğŸ§¬ BioX ALERT: {relevant_ticker} ğŸ§¬\n"
                        f"MOÅ»LIWY WYBUCH!\n"
                        f"ğŸ“° {headline}\n"
                        f"ğŸ¤– AI: {reason}"
                    )
                    append_scan_log(session, f"ğŸš€ {alert_msg}")
                    send_telegram_alert(alert_msg)
                    
                    # Oflagowanie (podbicie daty analizy)
                    session.execute(text("UPDATE phasex_candidates SET analysis_date = NOW() WHERE ticker = :t"), {'t': relevant_ticker})
                    session.commit()
                
                # Logujemy teÅ¼ "ciekawe" ale nie krytyczne, Å¼ebyÅ› widziaÅ‚ pracÄ™ AI
                elif sentiment != 'NEUTRAL':
                    append_scan_log(session, f"â„¹ï¸ BioX Info: {relevant_ticker} - {sentiment} ({reason})")

        except Exception as e:
            logger.error(f"BioX Live: BÅ‚Ä…d API: {e}")
            continue
        
        time.sleep(1.5) 

    # LOG KOÅƒCOWY (Podsumowanie cyklu)
    if processed_news_count > 0:
        end_msg = f"ğŸ BioX Agent: Przeanalizowano {processed_news_count} nowych newsÃ³w. AlertÃ³w: {alerts_sent}."
        append_scan_log(session, end_msg)
    else:
        # Dajemy znaÄ‡, Å¼e Å¼yjemy, ale nic nie znaleziono (cisza w eterze)
        pass # MoÅ¼na odkomentowaÄ‡ poniÅ¼szÄ… liniÄ™, jeÅ›li chcesz widzieÄ‡ log co 5 min nawet przy braku newsÃ³w
        # append_scan_log(session, "BioX Agent: Brak nowych wiadomoÅ›ci w tym cyklu.")

# ==================================================================
# CZÄ˜ÅšÄ† 2: HISTORICAL AUDIT (Dla Backtestu)
# ==================================================================

def run_historical_catalyst_scan(session: Session, api_client):
    """
    Analiza Wsteczna dla Backtestu.
    """
    logger.info("BioX History: Start analizy wstecznej...")
    append_scan_log(session, "ğŸ§¬ BioX History: Analiza katalizatorÃ³w dla historycznych pomp...")

    # Pobieramy kandydatÃ³w, ktÃ³rzy mieli pompÄ™ (tutaj pole last_pump_percent bÄ™dzie 0,
    # bo usunÄ™liÅ›my logikÄ™ ze skanera, wiÄ™c w nowym podejÅ›ciu ten moduÅ‚ bÄ™dzie czekaÅ‚
    # na dane z Backtest Engine, ktÃ³ry uzupeÅ‚ni historiÄ™ transakcji).
    
    # W tym momencie (po czystym skanie) ta funkcja moÅ¼e nie mieÄ‡ co robiÄ‡,
    # dopÃ³ki nie puÅ›cisz Backtestu, ktÃ³ry wygeneruje 'virtual_trades' z pompami.
    
    append_scan_log(session, "BioX History: Oczekiwanie na wyniki Backtestu (Symulacji Pomp).")
